"""
llm_loader2.py  
ä¸­éšŽç‰ˆæœ¬ï¼šä¿ç•™ PromptTemplateã€åˆ†æžåŠŸèƒ½ã€å®Œæ•´ PostgreSQL è¨˜æ†¶æž¶æ§‹
åˆªé™¤ UI èˆ‡æ—¥èªŒç›¸é—œç¨‹å¼
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from config2 import AppConfig
from prompt_templates2_pro import create_formatter
from memory_manager import MemoryManager  # âœ… ä½¿ç”¨åŽŸå§‹çš„ DB è¨˜æ†¶æž¶æ§‹


# =========================================================
# ðŸ§  æ¨¡åž‹è¼‰å…¥å™¨
# =========================================================
class ModelLoader:
    """æ ¹æ“š config è¼‰å…¥æ¨¡åž‹èˆ‡ tokenizer"""
    def __init__(self, config: AppConfig):
        self.config = config

    def load(self):
        """è¼‰å…¥æ¨¡åž‹èˆ‡ tokenizer"""
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=self.config.model.load_in_4bit,
            bnb_4bit_use_double_quant=self.config.model.bnb_4bit_use_double_quant,
            bnb_4bit_quant_type=self.config.model.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=self.config.model.bnb_4bit_compute_dtype
        )

        tokenizer = AutoTokenizer.from_pretrained(
            self.config.model.model_path,
            trust_remote_code=self.config.model.trust_remote_code
        )
        model = AutoModelForCausalLM.from_pretrained(
            self.config.model.model_path,
            quantization_config=bnb_config,
            trust_remote_code=self.config.model.trust_remote_code,
            device_map=self.config.model.device_map
        )
        return model, tokenizer


# =========================================================
# ðŸ¤– Qwen å°è©±ä»£ç†ï¼ˆä¸»æ ¸å¿ƒï¼‰
# =========================================================
class QwenAgent:
    """å…·å‚™ PromptTemplateã€åˆ†æžèˆ‡è³‡æ–™åº«è¨˜æ†¶åŠŸèƒ½çš„å°è©±ä»£ç†"""
    def __init__(self, model, tokenizer, config: AppConfig):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.prompt_formatter = create_formatter()
        self.memory = MemoryManager(model, tokenizer, config.memory)  # âœ… ä½¿ç”¨çœŸå¯¦ PostgreSQL è¨˜æ†¶ç®¡ç†å™¨
        self.last_analysis = {}

    # -----------------------------------------------------
    # ðŸŒ ä¸»æµç¨‹ï¼šå°è©±å›žè¦†
    # -----------------------------------------------------
    def process(self, messages):
        """ç”Ÿæˆå›žè¦†ã€åˆ†æžèˆ‡è¨˜æ†¶æ›´æ–°"""
        # 1ï¸âƒ£ æ ¼å¼åŒ–å°è©±å…§å®¹
        if self.config.prompt.use_socratic_template:
            current_stage = self.memory.current_stage
            formatted_messages = self.prompt_formatter.format_conversation(
                messages, use_template=True, stage=current_stage
            )
        else:
            formatted_messages = messages

        # 2ï¸âƒ£ å¥—ç”¨ chat template
        text = self.tokenizer.apply_chat_template(
            formatted_messages, tokenize=False, add_generation_prompt=True
        )
        #é€™é‚Šçš„tokenizerï¼Œæ˜¯autotokenizerï¼Œå› æ­¤å¥—ç”¨Qwenè®€å¾—æ‡‚çš„æ¨¡æ¿è®“Qwenè®€ä¸Šä¸‹æ–‡ã€‚

        # 3ï¸âƒ£ æ¨¡åž‹ç”Ÿæˆ
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        with torch.inference_mode():
            #inputçœŸæ­£è½‰æˆtoken+ç”Ÿæˆoutput
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.generation.max_new_tokens,
                temperature=self.config.generation.temperature,
                top_p=self.config.generation.top_p,
                repetition_penalty=self.config.generation.repetition_penalty,
            )
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )

        # 4ï¸âƒ£ è§£æžåˆ†æžçµæžœ
        self.last_analysis = self.extract_analysis(response)

        # 5ï¸âƒ£ æ›´æ–°è³‡æ–™åº«è¨˜æ†¶
        try:
            # å°‡ç”¨æˆ¶èˆ‡ AI å°è©±åŠ å…¥è¨˜æ†¶
            for msg in messages:
                self.memory.add_message(msg["role"], msg["content"])
            self.memory.add_message("assistant", response)
            # æ›´æ–°æƒ…ç·’ã€ä¿¡å¿µã€éšŽæ®µç­‰åˆ†æžè³‡è¨Š
            self.memory.update_analysis(self.last_analysis)
            # è‹¥ç‚ºçµæ¡ˆéšŽæ®µï¼Œè‡ªå‹•å„²å­˜ Session
            if self.memory.is_closure_stage(self.last_analysis):
                self.memory.save_session()
        except Exception as e:
            print(f"[Memory Error] ç„¡æ³•æ›´æ–°è³‡æ–™åº«è¨˜æ†¶: {str(e)}")

        # 6ï¸âƒ£ å›žå‚³ LLM å›žè¦†
        torch.cuda.empty_cache()
        return response

    # -----------------------------------------------------
    # ðŸ“Š åˆ†æžæ¨¡çµ„ï¼ˆç¶­æŒè¼•é‡ï¼‰
    # -----------------------------------------------------
    def extract_analysis(self, response: str):
        """æ ¹æ“š LLM å›žè¦†è§£æžæƒ…ç·’ã€ä¿¡å¿µã€éšŽæ®µ"""
        analysis = {"emotion": "æœªçŸ¥", "context": "æœªçŸ¥", "belief": "æœªçŸ¥", "stage": "æœªçŸ¥"}
        for key in analysis.keys():
            token = f"ã€{key}ã€‘"
            if token in response:
                try:
                    analysis[key] = response.split(token)[1].split("\n")[0].strip()
                except:
                    pass
        return analysis

    def get_analysis(self):
        """ä¾› Streamlit å‘¼å«é¡¯ç¤ºåˆ†æžå¡"""
        return self.last_analysis

    def get_memory_info(self):
        """å¾ž PostgreSQL æŠ“å–ç•¶å‰è¨˜æ†¶ç‹€æ…‹"""
        try:
            return self.memory.get_memory_summary()
        except Exception as e:
            return {"enabled": False, "error": str(e)}


# =========================================================
# ðŸ­ Agent å·¥å» ï¼ˆä¾è³´æ³¨å…¥ï¼‰
# =========================================================
class AgentFactory:
    """å»ºç«‹ Agent å¯¦ä¾‹"""
    @staticmethod
    def create_agent(config: AppConfig) -> QwenAgent:
        loader = ModelLoader(config)
        model, tokenizer = loader.load()
        return QwenAgent(model, tokenizer, config)
